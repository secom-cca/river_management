# app.py
from __future__ import annotations
import io
import re
from pathlib import Path
from functools import lru_cache
from typing import List, Dict, Any, Tuple

import numpy as np
import pandas as pd
import streamlit as st
from pysd import load, read_vensim


# =========================
# è¨­å®š
# =========================
DEFAULT_MODEL_PY  = Path("River_management_xls.py")
DEFAULT_MODEL_MDL = Path("River_management_xls.mdl")

# ãƒ¢ãƒ‡ãƒ«ã® GET XLS DATA ãŒå‚ç…§ã™ã‚‹ Excelï¼ˆVensim å´ã®ãƒ•ã‚¡ã‚¤ãƒ«åãƒ»ã‚·ãƒ¼ãƒˆåã«åˆã‚ã›ã‚‹ï¼‰
INPUT_XLSX_PATH = Path("input.xlsx")
INPUT_SHEET     = "input"

# NIES æœªæ¥æ°—å€™ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå€™è£œï¼ˆã©ã¡ã‚‰ã‹ã«ç½®ã„ã¦ã‚ã‚Œã°OKï¼‰
NIES_DIR_CANDIDATES = [Path("data/nies2020"), Path("data/nies")]


# ==== æµåŸŸãƒ—ãƒªã‚»ãƒƒãƒˆï¼ˆä»£è¡¨å€¤ã‚»ãƒƒãƒˆï¼‰ ====
PRESETS = {
    "ç­‘å¾Œå·æµåŸŸ": {
        "initial_dam_capacity": 74_200_000,      # m3
        "upstream_area": 157_585,                # ha
        "downstream_area": 143_951,              # ha
        "forest_area_ratio": 166_000/(198_500*0.9),
        "direct_discharge_ratio": 0.97,
        "current_highwater_discharge": 11_500,   # m3/s
        "paddy_field_ratio": 0.12,
    },
    "é•·è‰¯å·æµåŸŸ": {
        "initial_dam_capacity": 8_500_000,      # m3
        "upstream_area": 178_650,               # ha
        "downstream_area": 19_850,              # ha
        "forest_area_ratio": 0.92,
        "direct_discharge_ratio": 0.97,
        "current_highwater_discharge": 8_900,   # m3/s
        "paddy_field_ratio": 0.8,
    },
    "åˆ©æ ¹å·æµåŸŸï¼ˆä¾‹ï¼‰": {
        "initial_dam_capacity": 200_000_000,
        "upstream_area": 420_000,
        "downstream_area": 600_000,
        "forest_area_ratio": 0.65,
        "direct_discharge_ratio": 0.96,
        "current_highwater_discharge": 22_000,
        "paddy_field_ratio": 0.10,
    },
}

DEFAULT_RETURN_COLS = [
    "daily_total_gdp",
    "dam_storage",
    "downstream_storage",
    "upstream_storage",
    "river_discharge_downstream",
    "houses_damaged_by_inundation",
    "financial_damage_by_innundation",
    "financial_damage_by_flood",
]

PARAM_SPECS = {
    "daily_precipitation_future_ratio": dict(label="å°†æ¥é™æ°´è£œæ­£ï¼ˆÃ—ï¼‰", min=0.5, max=2.0, step=0.01, value=1.0),
    "dam_investment_amount":           dict(label="ãƒ€ãƒ æŠ•è³‡é¡ï¼ˆå††/å¹´ï¼‰",           min=0, max=1_000_000_000, step=1_000_000, value=0),
    "levee_investment_amount":         dict(label="å ¤é˜²æŠ•è³‡é¡ï¼ˆå††/å¹´ï¼‰",           min=0, max=100_000_000,  step=1_000_000,  value=0),
    "drainage_investment_amount":      dict(label="æ’æ°´èƒ½åŠ›æŠ•è³‡é¡ï¼ˆå††/å¹´ï¼‰",       min=0, max=10_000_000_000, step=100_000_000, value=0),
    "annual_paddy_dam_investment":     dict(label="ãŸã‚æ± ï¼ˆåœƒå ´ï¼‰æŠ•è³‡é¡ï¼ˆå††/å¹´ï¼‰",   min=0, max=50_000_000,   step=1_000_000,  value=1_000_000),
    "dam_investment_start_time":       dict(label="ãƒ€ãƒ æŠ•è³‡ é–‹å§‹æ™‚æœŸï¼ˆå¹´ï¼‰",        min=0, max=11, step=1, value=0),
    "levee_investment_start_time":     dict(label="å ¤é˜²æŠ•è³‡ é–‹å§‹æ™‚æœŸï¼ˆå¹´ï¼‰",        min=0, max=10, step=1, value=0),
    "eldery_people_ratio":             dict(label="é«˜é½¢è€…æ¯”ç‡", min=0.0, max=1.0, step=0.01, value=0.6),
    "capacity_building":               dict(label="é˜²ç½åŠ›ï¼ˆé¿é›£ç‡ä¿‚æ•°ï¼‰", min=0.0, max=1.0, step=0.05, value=0.5),
    "outflow_rate_of_residents":       dict(label="ä½æ°‘æµå‡ºç‡ï¼ˆ/æ—¥ï¼‰", min=0.0, max=0.1, step=0.0001, value=0.01/365),
    "inflow_rate_of_residents":        dict(label="ä½æ°‘æµå…¥ç‡ï¼ˆ/æ—¥ï¼‰", min=0.0, max=0.1, step=0.0001, value=0.01/365),
    "ratio_of_paddy_field_in_risky_area": dict(label="ãƒªã‚¹ã‚¯åŸŸã®åœƒå ´æ¯”ç‡", min=0.0, max=1.0, step=0.01, value=0.01),
    "paddy_field_ratio":                   dict(label="ä¸‹æµåŸŸã«ãŠã‘ã‚‹åœƒå ´æ¯”ç‡", min=0.0, max=1.0, step=0.01, value=0.12),

    # åœ°åŸŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    "initial_dam_capacity": dict(label="åˆæœŸãƒ€ãƒ å®¹é‡ï¼ˆmÂ³ï¼‰", min=0, max=500_000_000, step=100_000, value=74_200_000),
    "upstream_area":        dict(label="ä¸ŠæµåŸŸé¢ç©ï¼ˆhaï¼‰",  min=1_000, max=1_000_000, step=100, value=157_585),
    "downstream_area":      dict(label="ä¸‹æµåŸŸé¢ç©ï¼ˆhaï¼‰",  min=1_000, max=1_000_000, step=100, value=143_951),
    "forest_area_ratio":    dict(label="æ£®æ—é¢ç©æ¯”ï¼ˆ-ï¼‰",   min=0.0, max=1.0, step=0.001, value=166_000/(198_500*0.9)),
    "direct_discharge_ratio": dict(label="ç›´æ¥æµå‡ºæ¯”ï¼ˆ-ï¼‰", min=0.0, max=1.0, step=0.001, value=1 - 40/1950),
    "current_highwater_discharge": dict(label="è¨ˆç”»é«˜æ°´æµé‡ï¼ˆmÂ³/ç§’ï¼‰", min=0, max=30_000, step=100, value=11_500),
}


# =========================
# å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================
@lru_cache(maxsize=4)
def _load_model_from_file(model_py: str|None, model_mdl: str|None):
    if model_py and Path(model_py).exists():
        return load(model_py)
    if model_mdl and Path(model_mdl).exists():
        return read_vensim(model_mdl)
    raise FileNotFoundError("ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.py/.mdlï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

def _load_model_fresh(use_py_first: bool, model_py_path: str, model_mdl_path: str):
    py = Path(model_py_path)
    mdl = Path(model_mdl_path)
    if use_py_first and py.exists():
        return load(str(py))
    if (not use_py_first) and mdl.exists():
        return read_vensim(str(mdl))
    if py.exists():
        return load(str(py))
    if mdl.exists():
        return read_vensim(str(mdl))
    raise FileNotFoundError("ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

def _run_simulation(model, params: Dict[str, Any], timestamps: List[float], return_cols: List[str]) -> pd.DataFrame:
    """
    å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã¯ GET XLS DATA ã§ 'input.xlsx' ã‚’å‚ç…§ã—ã¦ã„ã‚‹å‰æï¼ˆdata=ã¯ä½¿ã‚ãªã„ï¼‰
    """
    try:
        return model.run(params=params, return_timestamps=timestamps, return_columns=return_cols)
    except Exception as e:
        st.warning(f"é¸æŠåˆ—ã®ä¸€éƒ¨ãŒè¦‹ã¤ã‹ã‚‰ãªã„å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€å…¨é‡å®Ÿè¡Œã—ã¦å†æŠ½å‡ºã—ã¾ã™ã€‚è©³ç´°: {e}")
        res = model.run(params=params, return_timestamps=timestamps)
        keep = [c for c in return_cols if c in res.columns]
        if missing := [c for c in return_cols if c not in res.columns]:
            st.warning(f"ä»¥ä¸‹ã®åˆ—ã¯ãƒ¢ãƒ‡ãƒ«ã«å­˜åœ¨ã—ã¾ã›ã‚“ã§ã—ãŸ: {missing}")
        return res[keep] if keep else res

def _build_model_datetime_index(res: pd.DataFrame, start_date: pd.Timestamp) -> pd.DataFrame:
    res = res.copy()
    res.index = pd.to_datetime([start_date + pd.Timedelta(days=int(t)) for t in res.index])
    res.index.name = "date"
    return res

# è¦³æ¸¬CSVï¼ˆæµé‡ï¼‰èª­ã¿è¾¼ã¿
def _read_observed_csv(file: io.BytesIO, date_col: str, flow_col: str, unit: str) -> pd.DataFrame:
    df = pd.read_csv(file)
    df[date_col] = pd.to_datetime(df[date_col])
    df = df[[date_col, flow_col]].rename(columns={date_col: "date", flow_col: "obs_flow"})
    if unit == "m3/s":
        df["obs_flow"] = df["obs_flow"] * 86400.0  # m3/s â†’ m3/day
    df = df.groupby("date", as_index=False).mean().set_index("date").sort_index()
    return df

# æŒ‡æ¨™ï¼†ãƒ©ã‚°
def _metrics(y_true: pd.Series, y_pred: pd.Series) -> Dict[str, float]:
    mask = ~(y_true.isna() | y_pred.isna())
    yt = y_true[mask].astype(float)
    yp = y_pred[mask].astype(float)
    if len(yt) == 0:
        return {}
    rmse = float(np.sqrt(np.mean((yp - yt) ** 2)))
    mae  = float(np.mean(np.abs(yp - yt)))
    bias = float(np.mean(yp - yt))
    corr = float(np.corrcoef(yt, yp)[0,1]) if len(yt)>1 else np.nan
    nse  = 1.0 - float(np.sum((yp-yt)**2) / np.sum((yt - yt.mean())**2)) if yt.nunique()>1 else np.nan
    return {"RMSE": rmse, "MAE": mae, "Mean Bias": bias, "Pearson r": corr, "NSE": nse}

def _best_lag(y_true: pd.Series, y_pred: pd.Series, max_lag_days: int) -> Tuple[int, float]:
    best_lag, best_r = 0, -np.inf
    for lag in range(-max_lag_days, max_lag_days+1):
        if lag > 0:
            r = y_true.corr(y_pred.shift(lag))
        elif lag < 0:
            r = y_true.shift(-lag).corr(y_pred)
        else:
            r = y_true.corr(y_pred)
        r = -1.0 if pd.isna(r) else float(r)
        if r > best_r:
            best_r, best_lag = r, lag
    return best_lag, best_r


# =========================
# NIES SSP CSV èª­ã¿è¾¼ã¿ï¼†æˆå½¢
# =========================
def _nies_csv_path(var: str, ssp_code: str|int) -> Path:
    fname = f"national_average_{var}_ssp{ssp_code}.csv"
    for base in NIES_DIR_CANDIDATES:
        p = base / fname
        if p.exists():
            return p
    # è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°å€™è£œãƒ‘ã‚¹ã‚’åˆ—æŒ™ã—ã¦ã‚¨ãƒ©ãƒ¼
    cand = ", ".join(str((base / fname).resolve()) for base in NIES_DIR_CANDIDATES)
    raise FileNotFoundError(f"{fname} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ¢ã—ãŸå ´æ‰€: {cand}")

def _parse_nies_csv(var: str, ssp_code: str|int) -> Tuple[pd.DataFrame, list[dict], list[str], list[int]]:
    """
    national_average_<var>_ssp<code>.csv ã‚’èª­ã¿è¾¼ã¿ã€åˆ—åã‹ã‚‰ model/year ã‚’æŠ½å‡º
    è¿”ã‚Šå€¤: (base_df(index=time), entries, models, years)
    """
    path = _nies_csv_path(var, ssp_code)
    df = pd.read_csv(path)
    if "time" not in df.columns:
        df = df.rename(columns={df.columns[0]: "time"})
    base_df = df.set_index("time")

    # ä¾‹: pr_MIROC6_ssp245_r1i1p1f1_2050
    pattern = re.compile(rf'^{re.escape(var)}_(?P<model>.+?)_ssp{ssp_code}_.+?_(?P<year>\d{{4}})$')
    entries, models, years = [], set(), set()
    for c in base_df.columns:
        m = pattern.match(str(c))
        if m:
            model = m.group("model"); year = int(m.group("year"))
            entries.append({"model": model, "year": year, "col": c})
            models.add(model); years.add(year)
    return base_df, entries, sorted(models), sorted(years)

def _extract_one_year(base_df: pd.DataFrame, entries: list[dict], model: str, year: int, var: str) -> pd.Series:
    for e in entries:
        if e["model"] == model and e["year"] == year:
            s = base_df[e["col"]].copy()
            s.index.name = "doy"  # 0..364/365
            s.name = var
            return s
    raise KeyError(f"{var}: {model} {year} ã®åˆ—ãŒã‚ã‚Šã¾ã›ã‚“")

def _drop_feb29_by_date_index(df: pd.DataFrame) -> pd.DataFrame:
    if isinstance(df.index, pd.DatetimeIndex):
        return df[~((df.index.month == 2) & (df.index.day == 29))]
    return df

def _clean_numeric(series: pd.Series, fallback: pd.Series|None=None) -> pd.Series:
    s = pd.to_numeric(series, errors="coerce")
    if s.isna().all() and fallback is not None:
        s = pd.to_numeric(fallback, errors="coerce")
    s = s.interpolate(limit_direction="both")
    s = s.bfill().ffill().fillna(0)
    s = s.replace([np.inf, -np.inf], 0)
    return s.astype(float)

def build_extdata_multi_year(ssp_code: str|int, model: str, start_year: int, n_years: int) -> pd.DataFrame:
    """
    æŒ‡å®šSSP/ãƒ¢ãƒ‡ãƒ«/é–‹å§‹å¹´/å¹´æ•°ã®é€£çµãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆNo., precipitation, temperature, tasmax, tasmin, rsds, dateï¼‰
    - é–æ—¥ã¯å‰Šé™¤ã—ã¦è©°ã‚ã‚‹ï¼ˆå¸¸ã« 365*n_years è¡Œï¼‰
    - æ¬ æ¸¬ã¯è£œé–“å¾Œã«å‰å¾Œè©°ã‚ã€ãªãŠæ®‹ã‚Œã° 0
    """
    pr_base, pr_ent, _, _     = _parse_nies_csv("pr",     ssp_code)
    tas_base, tas_ent, _, _   = _parse_nies_csv("tas",    ssp_code)
    tmax_base, tmax_ent, _, _ = _parse_nies_csv("tasmax", ssp_code)
    tmin_base, tmin_ent, _, _ = _parse_nies_csv("tasmin", ssp_code)
    rsds_base, rsds_ent, _, _ = _parse_nies_csv("rsds",   ssp_code)

    frames = []
    for y in range(start_year, start_year + n_years):
        pr    = _extract_one_year(pr_base,   pr_ent,   model, y, "precipitation")
        tas   = _extract_one_year(tas_base,  tas_ent,  model, y, "temperature")
        tasmx = _extract_one_year(tmax_base, tmax_ent, model, y, "tasmax")
        tasmn = _extract_one_year(tmin_base, tmin_ent, model, y, "tasmin")
        rsds  = _extract_one_year(rsds_base, rsds_ent, model, y, "rsds")

        start = pd.Timestamp(f"{y}-01-01")
        dates = [start + pd.Timedelta(days=int(d)) for d in pr.index]
        dfy = pd.DataFrame({
            "precipitation": pr.values,
            "temperature":   tas.values,
            "tasmax":        tasmx.values,
            "tasmin":        tasmn.values,
            "rsds":          rsds.values,
        }, index=pd.to_datetime(dates))
        frames.append(_drop_feb29_by_date_index(dfy))

    df = pd.concat(frames, axis=0)
    df.index.name = "date"

    # æ•°å€¤åŒ–ï¼†åŸ‹ã‚
    df["temperature"]   = _clean_numeric(df["temperature"])
    df["precipitation"] = _clean_numeric(df["precipitation"])
    df["tasmax"]        = _clean_numeric(df["tasmax"], fallback=df["temperature"])
    df["tasmin"]        = _clean_numeric(df["tasmin"], fallback=df["temperature"])
    df["rsds"]          = _clean_numeric(df["rsds"])

    df = df.reset_index()
    df.insert(0, "No.", np.arange(len(df), dtype=int))
    df = df[["No.", "precipitation", "temperature", "tasmax", "tasmin", "rsds", "date"]]
    return df

def write_input_excel_no_blank(table: pd.DataFrame, out_path: str|Path = INPUT_XLSX_PATH):
    """
    ã‚·ãƒ¼ãƒˆå 'input'ã€ç©ºæ¬„ãªã—ã§å‡ºåŠ›ã€‚
    """
    tbl = table.copy()
    for col in ["precipitation", "temperature", "tasmax", "tasmin", "rsds"]:
        if col not in tbl.columns:
            tbl[col] = 0.0
        tbl[col] = _clean_numeric(tbl[col])
    out_path = Path(out_path)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with pd.ExcelWriter(out_path, engine="openpyxl") as xw:
        tbl.to_excel(xw, sheet_name=INPUT_SHEET, index=False)

def _read_input_excel_table(file_or_path: io.BytesIO | str | Path) -> pd.DataFrame:
    """
    input.xlsx ã® 'input' ã‚·ãƒ¼ãƒˆã‚’ DataFrame ã¨ã—ã¦èª­ã¿è¾¼ã‚€
    å¿…é ˆåˆ—: No., precipitation, temperature, tasmax, tasmin, rsds, date
    """
    df = pd.read_excel(file_or_path, sheet_name=INPUT_SHEET)
    need = {"precipitation", "temperature", "tasmax", "tasmin", "rsds", "date"}
    missing = need - set(df.columns)
    if missing:
        raise ValueError(f"input.xlsx ã«å¿…è¦ãªåˆ—ãŒã‚ã‚Šã¾ã›ã‚“: {missing}")
    df["date"] = pd.to_datetime(df["date"])
    for col in ["precipitation", "temperature", "tasmax", "tasmin", "rsds"]:
        df[col] = _clean_numeric(df[col])
    # å¿µã®ãŸã‚ã†ã‚‹ã†æ—¥ãŒã‚ã‚Œã°å‰Šé™¤
    df = df.set_index("date")
    df = df[~((df.index.month == 2) & (df.index.day == 29))].reset_index()
    return df


# =========================
# UI
# =========================
st.set_page_config(page_title="River Management Simulator", layout="wide")
st.title("ğŸŒŠ River Management Simulator (PySD + Streamlit)")
st.caption("AMeDAS ã‚’ä½¿ã£ãŸéå»å†ç¾ï¼ˆè¦³æ¸¬ã¨æ¯”è¼ƒï¼‰ï¼‹ NIES/SSP Ã— 5 GCM ã®å°†æ¥è¨ˆç®—ã€‚")

with st.sidebar:
    st.header("1) ãƒ¢ãƒ‡ãƒ«èª­è¾¼")
    use_py_first = st.toggle("å¤‰æ›æ¸ˆã¿ .py ã‚’å„ªå…ˆã™ã‚‹", value=True)
    model_py_path  = st.text_input("ãƒ¢ãƒ‡ãƒ« .py ãƒ‘ã‚¹", str(DEFAULT_MODEL_PY))
    model_mdl_path = st.text_input("ãƒ¢ãƒ‡ãƒ« .mdl ãƒ‘ã‚¹", str(DEFAULT_MODEL_MDL))

    st.divider()
    st.header("2) AMeDAS å…¥åŠ›ï¼ˆéå»å†ç¾ãƒ»è¦³æ¸¬æ¯”è¼ƒï¼‰")
    amedas_xlsx = st.file_uploader("AMeDAS ã® input.xlsxï¼ˆã‚·ãƒ¼ãƒˆå inputï¼‰", type=["xlsx"])

    st.divider()
    st.header("3) å°†æ¥æ°—å€™ï¼ˆNIES national_average_*.csvï¼‰")
    ssp_code = st.selectbox("SSP é¸æŠ", options=["119", "126", "245", "585"], index=2)  # æ—¢å®š: 245
    start_year = st.number_input("é–‹å§‹å¹´", value=2015, step=1, min_value=1900, max_value=2100)
    n_years    = st.number_input("å¹´æ•°ï¼ˆ1å¹´ä»¥ä¸Šå¯ï¼‰", value=1, step=1, min_value=1, max_value=300)

    st.caption("â€» CSV ã¯ data/nies2020/ ã¾ãŸã¯ data/nies/ ã«é…ç½®ã€‚ä¾‹: national_average_pr_ssp245.csv")

    st.divider()
    st.header("4) æµåŸŸãƒ—ãƒªã‚»ãƒƒãƒˆï¼†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    preset_name = st.selectbox("ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’é¸æŠ", list(PRESETS.keys()))
    if "params_ui" not in st.session_state:
        st.session_state["params_ui"] = {}
    if st.button("ã“ã®ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã¸åæ˜ "):
        for k, v in PRESETS[preset_name].items():
            st.session_state["params_ui"][k] = v
        st.success(f"ã€Œ{preset_name}ã€ã®å€¤ã‚’åæ˜ ã—ã¾ã—ãŸ")

    ui_values = {}
    for name, spec in PARAM_SPECS.items():
        default_value = st.session_state["params_ui"].get(name, spec.get("value", 0))
        ui_values[name] = st.slider(
            spec["label"],
            min_value=spec.get("min", 0.0),
            max_value=spec.get("max", 1.0),
            step=spec.get("step", 0.01),
            value=default_value,
            key=f"param_{name}",
            format="%.6f" if spec.get("step", 1) < 1 else "%d"
        )
        st.session_state["params_ui"][name] = ui_values[name]
    params = ui_values

# ãƒ¢ãƒ‡ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
try:
    _ = _load_model_from_file(
        model_py_path if use_py_first else "",
        model_mdl_path if not use_py_first else "",
    )
except Exception as e:
    st.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
    st.stop()

# å‡ºåŠ›åˆ—
with st.container():
    st.header("5) è¡¨ç¤ºã—ãŸã„ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›")
    return_cols = st.multiselect(
        "å¤‰æ•°ï¼ˆã‚¹ãƒãƒ¼ã‚¯ã‚±ãƒ¼ã‚¹ï¼‰",
        DEFAULT_RETURN_COLS,
        default=DEFAULT_RETURN_COLS
    )

# è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ï¼ˆæµé‡ï¼‰
st.divider()
st.header("6) è¦³æ¸¬æµé‡ï¼ˆAMeDASå†ç¾ã¨ã®æ¯”è¼ƒç”¨ãƒ»ä»»æ„ï¼‰")
st.caption("CSV ä¾‹: date, flowï¼ˆm3/s or m3/dayï¼‰â€” ãƒ¢ãƒ‡ãƒ«ã®ä¸‹æµæµé‡ã¨æ¯”è¼ƒã—ã¾ã™ã€‚")
obs_file = st.file_uploader("è¦³æ¸¬CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["csv"], key="obs")
c1, c2, c3 = st.columns(3)
with c1:
    obs_date_col = st.text_input("æ—¥ä»˜åˆ—å", value="date")
with c2:
    obs_flow_col = st.text_input("æµé‡åˆ—å", value="flow")
with c3:
    obs_unit = st.selectbox("è¦³æ¸¬æµé‡ã®å˜ä½", options=["m3/day", "m3/s"], index=1)
max_lag_days = st.slider("å½¢çŠ¶æ¯”è¼ƒã®è¨±å®¹ãƒ©ã‚°ï¼ˆæ—¥ï¼‰", min_value=0, max_value=14, value=5)

# å®Ÿè¡Œãƒœã‚¿ãƒ³
run_btn = st.button("â–¶ AMeDASå†ç¾ï¼ˆè¦³æ¸¬æ¯”è¼ƒï¼‰ï¼‹ 5 GCM å°†æ¥è¨ˆç®— ã‚’å®Ÿè¡Œ", type="primary")


# =========================
# å®Ÿè¡Œ
# =========================
if run_btn:
    amedas_result: pd.DataFrame | None = None
    gcm_results: Dict[str, pd.DataFrame] = {}

    # ---------- AMeDAS å†ç¾ï¼ˆè¦³æ¸¬æ¯”è¼ƒï¼‰ ----------
    with st.spinner("AMeDAS ã‚’ç”¨ã„ãŸå†ç¾è¨ˆç®—ã‚’å®Ÿæ–½ä¸­..."):
        if amedas_xlsx is not None:
            try:
                # AMeDAS input.xlsx ã‚’ä¿å­˜ã—ã¦ãƒ¢ãƒ‡ãƒ«ã«èª­ã¾ã›ã‚‹
                with open(INPUT_XLSX_PATH, "wb") as f:
                    f.write(amedas_xlsx.getbuffer())

                # ãƒ†ãƒ¼ãƒ–ãƒ«èª­ã¿è¾¼ã¿ï¼ˆé–‹å§‹æ—¥ãƒ»æ—¥æ•°ã‚’å–å¾—ï¼‰
                am_tbl = _read_input_excel_table(amedas_xlsx)
                start_dt_amedas = pd.to_datetime(am_tbl["date"].iloc[0])
                n_days_amedas = len(am_tbl)
                timestamps_amedas = list(range(n_days_amedas))

                # ãƒ¢ãƒ‡ãƒ«ã‚’æ–°è¦ãƒ­ãƒ¼ãƒ‰ï¼ˆæ¯å›ï¼‰
                model_amedas = _load_model_fresh(use_py_first, str(model_py_path), str(model_mdl_path))

                sim_params_amedas = params.copy()
                for k, v in (("initial_time", 0), ("final_time", n_days_amedas - 1), ("time_step", 1)):
                    if hasattr(model_amedas.components, k):
                        sim_params_amedas[k] = v

                res_amedas = _run_simulation(
                    model_amedas,
                    params=sim_params_amedas,
                    timestamps=timestamps_amedas,
                    return_cols=list(set(return_cols))
                )
                amedas_result = _build_model_datetime_index(res_amedas, start_dt_amedas)
                st.success("AMeDAS å†ç¾ã®ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã‚’å¾—ã¾ã—ãŸã€‚")

            except Exception as e:
                st.error(f"AMeDAS å†ç¾ã§ã‚¨ãƒ©ãƒ¼: {e}")
                amedas_result = None
        else:
            st.info("AMeDAS ã® input.xlsx ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€å†ç¾è¨ˆç®—ã¨è¦³æ¸¬æ¯”è¼ƒãŒæœ‰åŠ¹ã«ãªã‚Šã¾ã™ã€‚")

    # AMeDAS vs è¦³æ¸¬ æ¯”è¼ƒ
    if amedas_result is not None:
        st.subheader("ğŸ†š AMeDAS å†ç¾çµæœ Ã— è¦³æ¸¬æµé‡ ã®æ¯”è¼ƒï¼ˆriver_discharge_downstreamï¼‰")
        if obs_file is not None:
            try:
                obs_df = _read_observed_csv(obs_file, obs_date_col, obs_flow_col, obs_unit)
            except Exception as e:
                st.error(f"è¦³æ¸¬CSVã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
                obs_df = None
        else:
            obs_df = None

        if obs_df is not None:
            if "river_discharge_downstream" in amedas_result.columns:
                merged = pd.concat(
                    [amedas_result["river_discharge_downstream"].rename("model_flow"), obs_df["obs_flow"]],
                    axis=1
                ).dropna()
                if not merged.empty:
                    lag, r_at_lag = _best_lag(merged["obs_flow"], merged["model_flow"], max_lag_days)
                    st.caption(f"æœ€é©ãƒ©ã‚°ï¼ˆæ—¥ï¼‰: {lag}ï¼ˆç›¸é–¢ {r_at_lag:.3f}ï¼‰")

                    plot_target = "model_flow" if lag == 0 else "model_flow_lag"
                    if lag != 0:
                        merged["model_flow_lag"] = merged["model_flow"].shift(lag)
                        merged = merged.dropna()

                    # æŒ‡æ¨™è¡¨ç¤º
                    scores = _metrics(merged["obs_flow"], merged[plot_target])
                    if scores:
                        c1, c2, c3, c4, c5 = st.columns(5)
                        c1.metric("RMSE (mÂ³/æ—¥)", f"{scores['RMSE']:.2f}")
                        c2.metric("MAE (mÂ³/æ—¥)",  f"{scores['MAE']:.2f}")
                        c3.metric("Bias (mÂ³/æ—¥)", f"{scores['Mean Bias']:.2f}")
                        c4.metric("ç›¸é–¢ r",       f"{scores['Pearson r']:.3f}")
                        c5.metric("NSE",          f"{scores['NSE']:.3f}")

                    # å®Ÿå€¤é‡ã­æã
                    st.markdown("**é‡ã­æãï¼ˆå®Ÿå€¤ï¼‰**")
                    show_df = merged[[plot_target, "obs_flow"]].rename(columns={plot_target: "model(AMeDAS)", "obs_flow": "observed"})
                    st.line_chart(show_df, height=320, use_container_width=True)

                    # æ¨™æº–åŒ–ï¼ˆå½¢çŠ¶æ¯”è¼ƒï¼‰
                    st.markdown("**å½¢çŠ¶æ¯”è¼ƒï¼ˆæ¨™æº–åŒ–ï¼šå¹³å‡0ãƒ»åˆ†æ•£1ï¼‰**")
                    z = show_df.apply(lambda s: (s - s.mean()) / (s.std() if s.std()!=0 else 1))
                    st.line_chart(z, height=320, use_container_width=True)

                    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                    with st.expander("ğŸ“¥ AMeDASå†ç¾Ã—è¦³æ¸¬ ã®CSVã‚’ä¿å­˜"):
                        out = amedas_result.copy()
                        out["obs_flow"] = obs_df["obs_flow"]
                        out["model_flow"] = amedas_result["river_discharge_downstream"]
                        if lag != 0:
                            out["model_flow_lag"] = out["model_flow"].shift(lag)
                        csv = out.to_csv(index_label="date").encode("utf-8")
                        st.download_button(
                            "AMeDASå†ç¾Ã—è¦³æ¸¬ï¼ˆCSVï¼‰",
                            data=csv,
                            file_name="amedas_baseline_vs_observed.csv",
                            mime="text/csv"
                        )
                else:
                    st.info("AMeDAS å†ç¾ã¨è¦³æ¸¬ã®é‡ãªã‚ŠæœŸé–“ãŒã‚ã‚Šã¾ã›ã‚“ã€‚æ—¥ä»˜ã®ç¯„å›²ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
            else:
                st.info("ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã« 'river_discharge_downstream' ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å‡ºåŠ›å¤‰æ•°ã®é¸æŠã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
        else:
            st.info("è¦³æ¸¬CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€AMeDASå†ç¾ã¨ã®æ¯”è¼ƒãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")

    # ---------- NIES/SSP Ã— 5 GCMï¼ˆå°†æ¥è¨ˆç®—ï¼‰ ----------
    with st.spinner("NIES/SSP å…¥åŠ›ã§ 5 GCM ã®å°†æ¥è¨ˆç®—ã‚’å®Ÿæ–½ä¸­..."):
        try:
            _, pr_entries, pr_models, _ = _parse_nies_csv("pr", ssp_code)
        except Exception as e:
            st.exception(e)
            st.stop()

        if len(pr_models) < 5:
            st.warning(f"ã“ã® SSP ã«å«ã¾ã‚Œã‚‹ GCM ãŒ 5 æœªæº€ã§ã™: {pr_models}")
        models_to_run = pr_models[:5]

        start_dt_nies = pd.Timestamp(f"{int(start_year)}-01-01")
        for gcm in models_to_run:
            try:
                in_table = build_extdata_multi_year(ssp_code, gcm, int(start_year), int(n_years))
                write_input_excel_no_blank(in_table, INPUT_XLSX_PATH)

                model_gcm = _load_model_fresh(use_py_first, str(model_py_path), str(model_mdl_path))

                n_days = len(in_table)
                timestamps = list(range(n_days))
                sim_params = params.copy()
                for k, v in (("initial_time", 0), ("final_time", n_days - 1), ("time_step", 1)):
                    if hasattr(model_gcm.components, k):
                        sim_params[k] = v

                res = _run_simulation(
                    model_gcm,
                    params=sim_params,
                    timestamps=timestamps,
                    return_cols=list(set(return_cols))
                )
                res = _build_model_datetime_index(res, start_dt_nies)
                gcm_results[gcm] = res

            except Exception as e:
                st.error(f"{gcm} ã®å®Ÿè¡Œã§ã‚¨ãƒ©ãƒ¼: {e}")

    if gcm_results:
        st.success("5 GCM ã®è¨ˆç®—ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        st.write("å¯¾è±¡ GCM:", ", ".join(gcm_results.keys()))

        # ä¸‹æµæµé‡ã®é‡ã­æã
        target_var = "river_discharge_downstream"
        if all((target_var in df.columns) for df in gcm_results.values()):
            st.subheader("ğŸ“ˆ ä¸‹æµæµé‡ï¼ˆriver_discharge_downstreamï¼‰â€” 5 GCM é‡ã­æãï¼ˆå°†æ¥ï¼‰")
            plot_df = pd.concat(
                [df[target_var].rename(f"{target_var} ({gcm})") for gcm, df in gcm_results.items()],
                axis=1
            )
            st.line_chart(plot_df, height=360, use_container_width=True)

        # å¤‰æ•°ã”ã¨ã®ã‚¿ãƒ–
        st.subheader("ğŸ“Š å¤‰æ•°ã”ã¨ã®æ™‚ç³»åˆ—ï¼ˆGCMåˆ¥ã‚¿ãƒ–ï¼‰")
        for var in return_cols:
            st.markdown(f"**{var}**")
            tabs = st.tabs(list(gcm_results.keys()))
            for (gcm, df), tab in zip(gcm_results.items(), tabs):
                with tab:
                    if var in df.columns:
                        st.line_chart(df[[var]], height=260, use_container_width=True)
                    else:
                        st.info(f"{gcm}: å¤‰æ•° {var} ã¯å‡ºåŠ›ã«å­˜åœ¨ã—ã¾ã›ã‚“")

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        st.subheader("ğŸ’¾ å°†æ¥è¨ˆç®—ã®çµæœãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        for gcm, df in gcm_results.items():
            csv_bytes = df.to_csv(index_label="date").encode("utf-8")
            st.download_button(
                f"{gcm} ã®å‡ºåŠ›CSVã‚’ä¿å­˜",
                data=csv_bytes,
                file_name=f"simulation_output_{gcm}_ssp{ssp_code}_{start_year}_{int(n_years)}y.csv",
                mime="text/csv"
            )

        if all((target_var in df.columns) for df in gcm_results.values()):
            river_df = pd.concat(
                [df[target_var].rename(gcm) for gcm, df in gcm_results.items()],
                axis=1
            )
            csv_bytes = river_df.to_csv(index_label="date").encode("utf-8")
            st.download_button(
                "river_discharge_downstreamï¼ˆ5 GCMæ¨ªæŒã¡ï¼‰ã‚’ä¿å­˜",
                data=csv_bytes,
                file_name=f"river_discharge_5gcm_ssp{ssp_code}_{start_year}_{int(n_years)}y.csv",
                mime="text/csv"
            )

    # AMeDAS å†ç¾å‡ºåŠ›ã®ä¿å­˜
    if amedas_result is not None:
        st.subheader("ğŸ’¾ AMeDAS å†ç¾å‡ºåŠ›ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        csv_bytes = amedas_result.to_csv(index_label="date").encode("utf-8")
        st.download_button(
            "AMeDAS å†ç¾å‡ºåŠ›CSVã‚’ä¿å­˜",
            data=csv_bytes,
            file_name=f"simulation_output_AMeDAS_baseline.csv",
            mime="text/csv"
        )


st.divider()
with st.expander("ğŸ§© ãƒ’ãƒ³ãƒˆ & ãƒ¡ãƒ¢"):
    st.markdown("""
- **AMeDAS å†ç¾**: `input.xlsx`ï¼ˆã‚·ãƒ¼ãƒˆå `input`ã€åˆ— `No., precipitation, temperature, tasmax, tasmin, rsds, date`ï¼‰ã‚’ã‚¢ãƒƒãƒ—ã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ã‚’ãã®å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã§é§†å‹•ã—ã€**è¦³æ¸¬æµé‡ CSV** ã¨æ¯”è¼ƒï¼ˆRMSE/MAE/Bias/r/NSEã€ãƒ©ã‚°æœ€é©åŒ–ï¼‰ã—ã¾ã™ã€‚
- **å°†æ¥è¨ˆç®—**: NIES ã® `national_average_<var>_ssp<code>.csv` ã‹ã‚‰ 5 GCM ã‚’è‡ªå‹•é¸æŠã€‚æœŸé–“ã¯ã€Œé–‹å§‹å¹´ï¼‹å¹´æ•°ã€ã€‚é–æ—¥ã¯å‰Šé™¤ã—ã¦è©°ã‚ã€**ç©ºæ¬„ã¯ä½œã‚‰ãš** `input.xlsx` ã‚’éƒ½åº¦ç”Ÿæˆã—ã¾ã™ã€‚
- NIES ãƒ‡ãƒ¼ã‚¿ã®å ´æ‰€ã¯ `data/nies2020/` ã¾ãŸã¯ `data/nies/` ã®ã©ã¡ã‚‰ã‹ã§OKã€‚
- èµ·å‹•: `pip install streamlit pysd numpy pandas openpyxl` â†’ `streamlit run app.py`
    """)
