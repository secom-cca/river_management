# app.py
from __future__ import annotations
import io
from pathlib import Path
from functools import lru_cache
from typing import List, Dict, Any, Tuple
import numpy as np
import pandas as pd
import streamlit as st
from pysd import load, read_vensim

# =========================
# è¨­å®š
# =========================
DEFAULT_MODEL_PY  = Path("River_management_xls.py")
DEFAULT_MODEL_MDL = Path("River_management_xls.mdl")
DEFAULT_XLS_NAME  = "jma_kurume_2023.xls"   # ãƒ¢ãƒ‡ãƒ«å†… ExtData ãŒå‚ç…§ã™ã‚‹æ—¢å®šå

# ==== æµåŸŸãƒ—ãƒªã‚»ãƒƒãƒˆï¼ˆä»£è¡¨å€¤ã‚»ãƒƒãƒˆï¼‰ ====
# å¿…è¦ã«å¿œã˜ã¦å€¤ã‚’å®Ÿæ¸¬ãƒ»è³‡æ–™ã®ä»£è¡¨å€¤ã«å·®ã—æ›¿ãˆã¦ãã ã•ã„
PRESETS = {
    "ç­‘å¾Œå·æµåŸŸ": {
        "initial_dam_capacity": 74_200_000,      # m3
        "upstream_area": 157_585,                # ha
        "downstream_area": 143_951,              # ha
        "forest_area_ratio": 166_000/(198_500*0.9),
        "direct_discharge_ratio": 0.97,
        "current_highwater_discharge": 11_500,   # m3/s
        "paddy_field_ratio": 0.12,
    },
    "é•·è‰¯å·æµåŸŸ": {
        "initial_dam_capacity": 8_500_000,      # m3
        "upstream_area": 178_650,                # ha
        "downstream_area": 19_850,              # ha
        "forest_area_ratio": 0.92,
        "direct_discharge_ratio": 0.97,
        "current_highwater_discharge": 8_900,   # m3/s
        "paddy_field_ratio": 0.8,
    },
    "åˆ©æ ¹å·æµåŸŸï¼ˆä¾‹ï¼‰": {
        "initial_dam_capacity": 200_000_000,
        "upstream_area": 420_000,
        "downstream_area": 600_000,
        "forest_area_ratio": 0.65,
        "direct_discharge_ratio": 0.96,
        "current_highwater_discharge": 22_000,
        "paddy_field_ratio": 0.10,
    },
}



DEFAULT_RETURN_COLS = [
    "daily_total_gdp",
    "dam_storage",
    "downstream_storage",
    "upstream_storage",
    "river_discharge_downstream",
    "houses_damaged_by_inundation",
    "financial_damage_by_innundation",
    "financial_damage_by_flood",
]

PARAM_SPECS = {
    "daily_precipitation_future_ratio": dict(label="å°†æ¥é™æ°´è£œæ­£ï¼ˆÃ—ï¼‰", min=0.5, max=2.0, step=0.01, value=1.0),
    "dam_investment_amount":           dict(label="ãƒ€ãƒ æŠ•è³‡é¡ï¼ˆå††/å¹´ï¼‰",           min=0, max=1_000_000_000, step=1_000_000, value=0),
    "levee_investment_amount":         dict(label="å ¤é˜²æŠ•è³‡é¡ï¼ˆå††/å¹´ï¼‰",           min=0, max=100_000_000,  step=1_000_000,  value=0),
    "drainage_investment_amount":      dict(label="æ’æ°´èƒ½åŠ›æŠ•è³‡é¡ï¼ˆå††/å¹´ï¼‰",       min=0, max=10_000_000_000, step=100_000_000, value=0),
    "annual_paddy_dam_investment":     dict(label="ãŸã‚æ± ï¼ˆåœƒå ´ï¼‰æŠ•è³‡é¡ï¼ˆå††/å¹´ï¼‰",   min=0, max=50_000_000,   step=1_000_000,  value=1_000_000),
    "dam_investment_start_time":       dict(label="ãƒ€ãƒ æŠ•è³‡ é–‹å§‹æ™‚æœŸï¼ˆå¹´ï¼‰",        min=0, max=11, step=1, value=0),
    "levee_investment_start_time":     dict(label="å ¤é˜²æŠ•è³‡ é–‹å§‹æ™‚æœŸï¼ˆå¹´ï¼‰",        min=0, max=10, step=1, value=0),
    "eldery_people_ratio":             dict(label="é«˜é½¢è€…æ¯”ç‡", min=0.0, max=1.0, step=0.01, value=0.6),
    "capacity_building":               dict(label="é˜²ç½åŠ›ï¼ˆé¿é›£ç‡ä¿‚æ•°ï¼‰", min=0.0, max=1.0, step=0.05, value=0.5),
    "outflow_rate_of_residents":       dict(label="ä½æ°‘æµå‡ºç‡ï¼ˆ/æ—¥ï¼‰", min=0.0, max=0.1, step=0.0001, value=0.01/365),
    "inflow_rate_of_residents":        dict(label="ä½æ°‘æµå…¥ç‡ï¼ˆ/æ—¥ï¼‰", min=0.0, max=0.1, step=0.0001, value=0.01/365),
    "ratio_of_paddy_field_in_risky_area": dict(label="ãƒªã‚¹ã‚¯åŸŸã®åœƒå ´æ¯”ç‡", min=0.0, max=1.0, step=0.01, value=0.01),
    "paddy_field_ratio":                   dict(label="ä¸‹æµåŸŸã«ãŠã‘ã‚‹åœƒå ´æ¯”ç‡", min=0.0, max=1.0, step=0.01, value=0.12),

    # --- åœ°åŸŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæ–°è¦è¿½åŠ ï¼‰ ---
    # Initial dam capacity (m3)
    "initial_dam_capacity": dict(
        label="åˆæœŸãƒ€ãƒ å®¹é‡ï¼ˆmÂ³ï¼‰",
        min=0, max=500_000_000, step=100_000, value=74_200_000
    ),
    # Upstream area (ha)
    "upstream_area": dict(
        label="ä¸ŠæµåŸŸé¢ç©ï¼ˆhaï¼‰",
        min=1_000, max=1_000_000, step=100, value=157_585
    ),
    # Downstream area (ha)
    "downstream_area": dict(
        label="ä¸‹æµåŸŸé¢ç©ï¼ˆhaï¼‰",
        min=1_000, max=1_000_000, step=100, value=143_951
    ),
    # Forest area ratio (-)
    "forest_area_ratio": dict(
        label="æ£®æ—é¢ç©æ¯”ï¼ˆ-ï¼‰",
        min=0.0, max=1.0, step=0.001, value=166_000/(198_500*0.9)  # â‰ˆ0.93
    ),
    # Direct discharge ratio (-)
    "direct_discharge_ratio": dict(
        label="ç›´æ¥æµå‡ºæ¯”ï¼ˆ-ï¼‰",
        min=0.0, max=1.0, step=0.001, value=1 - 40/1950  # â‰ˆ0.9795
    ),
    # "Current high-water discharge" (m3/s)
    "current_highwater_discharge": dict(
        label="è¨ˆç”»é«˜æ°´æµé‡ï¼ˆmÂ³/ç§’ï¼‰",
        min=0, max=30_000, step=100, value=11_500
    ),
}

# =========================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================
@lru_cache(maxsize=4)
def _load_model_from_file(model_py: str|None, model_mdl: str|None):
    if model_py and Path(model_py).exists():
        return load(model_py)
    if model_mdl and Path(model_mdl).exists():
        return read_vensim(model_mdl)
    raise FileNotFoundError("ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.py/.mdlï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

def _ensure_extdata_file(uploaded: io.BytesIO|None, expected_name: str):
    if uploaded is not None:
        with open(expected_name, "wb") as f:
            f.write(uploaded.getbuffer())

def _run_simulation(model, params: Dict[str, Any], timestamps: List[float], return_cols: List[str]) -> pd.DataFrame:
    try:
        return model.run(params=params, return_timestamps=timestamps, return_columns=return_cols)
    except Exception as e:
        st.warning(f"é¸æŠåˆ—ã®ä¸€éƒ¨ãŒè¦‹ã¤ã‹ã‚‰ãªã„å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€å…¨é‡å®Ÿè¡Œã—ã¦å†æŠ½å‡ºã—ã¾ã™ã€‚è©³ç´°: {e}")
        res = model.run(params=params, return_timestamps=timestamps)
        keep = [c for c in return_cols if c in res.columns]
        if missing := [c for c in return_cols if c not in res.columns]:
            st.warning(f"ä»¥ä¸‹ã®åˆ—ã¯ãƒ¢ãƒ‡ãƒ«ã«å­˜åœ¨ã—ã¾ã›ã‚“ã§ã—ãŸ: {missing}")
        return res[keep] if keep else res

def _build_model_datetime_index(res: pd.DataFrame, start_date: pd.Timestamp) -> pd.DataFrame:
    res = res.copy()
    res.index = pd.to_datetime([start_date + pd.Timedelta(days=int(t)) for t in res.index])
    res.index.name = "date"
    return res

def _read_observed_csv(file: io.BytesIO, date_col: str, flow_col: str, unit: str) -> pd.DataFrame:
    df = pd.read_csv(file)
    df[date_col] = pd.to_datetime(df[date_col])
    df = df[[date_col, flow_col]].rename(columns={date_col: "date", flow_col: "obs_flow"})
    if unit == "m3/s":
        df["obs_flow"] = df["obs_flow"] * 86400.0  # â†’ m3/day ã«å¤‰æ›
    # æ—¥å˜ä½ã«æ•´å½¢ï¼ˆé‡è¤‡ãŒã‚ã‚Œã°å¹³å‡ï¼‰
    df = df.groupby("date", as_index=False).mean().set_index("date").sort_index()
    return df

def _metrics(y_true: pd.Series, y_pred: pd.Series) -> Dict[str, float]:
    mask = ~(y_true.isna() | y_pred.isna())
    yt = y_true[mask].astype(float)
    yp = y_pred[mask].astype(float)
    if len(yt) == 0:
        return {}
    rmse = float(np.sqrt(np.mean((yp - yt) ** 2)))
    mae  = float(np.mean(np.abs(yp - yt)))
    bias = float(np.mean(yp - yt))
    corr = float(np.corrcoef(yt, yp)[0,1]) if len(yt)>1 else np.nan
    nse  = 1.0 - float(np.sum((yp-yt)**2) / np.sum((yt - yt.mean())**2)) if yt.nunique()>1 else np.nan
    return {"RMSE": rmse, "MAE": mae, "Mean Bias": bias, "Pearson r": corr, "NSE": nse}

def _best_lag(y_true: pd.Series, y_pred: pd.Series, max_lag_days: int) -> Tuple[int, float]:
    """ ç›¸é–¢ãŒæœ€å¤§ã¨ãªã‚‹ãƒ©ã‚°ï¼ˆæ—¥ï¼‰ã¨ç›¸é–¢å€¤ï¼ˆå½¢çŠ¶æ¯”è¼ƒç”¨ï¼‰ """
    best_lag, best_r = 0, -np.inf
    for lag in range(-max_lag_days, max_lag_days+1):
        if lag > 0:
            r = y_true.corr(y_pred.shift(lag))
        elif lag < 0:
            r = y_true.shift(-lag).corr(y_pred)
        else:
            r = y_true.corr(y_pred)
        r = -1.0 if pd.isna(r) else float(r)
        if r > best_r:
            best_r, best_lag = r, lag
    return best_lag, best_r

# =========================
# UI
# =========================
st.set_page_config(page_title="River Management Simulator", layout="wide")
st.title("ğŸŒŠ River Management Simulator (PySD + Streamlit)")
st.caption("Vensimãƒ¢ãƒ‡ãƒ«ã‚’PySDã§å®Ÿè¡Œã—ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è§¦ã£ã¦çµæœã‚’å¯è¦–åŒ–ã€‚è¦³æ¸¬ãƒ•ãƒ­ãƒ¼ã¨ã®å½¢çŠ¶æ¯”è¼ƒã‚‚ã§ãã¾ã™ã€‚")

with st.sidebar:
    st.header("1) ãƒ¢ãƒ‡ãƒ«èª­è¾¼")
    use_py_first = st.toggle("å¤‰æ›æ¸ˆã¿ .py ã‚’å„ªå…ˆã™ã‚‹", value=True)
    model_py_path  = st.text_input("ãƒ¢ãƒ‡ãƒ« .py ãƒ‘ã‚¹", str(DEFAULT_MODEL_PY))
    model_mdl_path = st.text_input("ãƒ¢ãƒ‡ãƒ« .mdl ãƒ‘ã‚¹", str(DEFAULT_MODEL_MDL))

    st.markdown("**GET XLS DATAï¼ˆå¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ï¼‰**")
    up = st.file_uploader(f"Excelï¼ˆ{DEFAULT_XLS_NAME} ã¨ã—ã¦ä¿å­˜ï¼‰", type=["xls", "xlsx"])

    st.divider()
    st.header("2) ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ™‚é–“")
    start_date = st.date_input("ãƒ¢ãƒ‡ãƒ«é–‹å§‹æ—¥ï¼ˆä¾‹ï¼š2023-01-01ï¼‰", pd.to_datetime("2023-01-01"))
    init_time = st.number_input("INITIAL TIME (day)", value=0, step=1)
    final_time = st.number_input("FINAL TIME (day)", value=364, step=1, min_value=init_time)
    time_step = st.number_input("TIME STEP (day)", value=1, step=1, min_value=1)
    timestamps = list(range(int(init_time), int(final_time)+1, int(time_step)))

with st.sidebar:
    st.header("3) æµåŸŸãƒ—ãƒªã‚»ãƒƒãƒˆ")
    preset_name = st.selectbox("ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’é¸æŠ", list(PRESETS.keys()))

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’åˆæœŸåŒ–ï¼ˆåˆå›ã®ã¿ï¼‰
    if "params_ui" not in st.session_state:
        st.session_state["params_ui"] = {}

    # é©ç”¨ãƒœã‚¿ãƒ³ã§ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼/æ•°å€¤å…¥åŠ›ã¸åæ˜ 
    if st.button("ã“ã®ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã¸åæ˜ "):
        chosen = PRESETS[preset_name]
        for k, v in chosen.items():
            st.session_state["params_ui"][k] = v
        st.success(f"ã€Œ{preset_name}ã€ã®å€¤ã‚’åæ˜ ã—ã¾ã—ãŸ")

    st.divider()
    st.header("4) ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆä¸Šæ›¸ãï¼‰")

    ui_values = {}
    for name, spec in PARAM_SPECS.items():
        label = spec["label"]
        # preset åæ˜ å¾Œã®å€¤ï¼ˆãªã‘ã‚Œã° spec ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        default_value = st.session_state["params_ui"].get(name, spec.get("value", 0))

        # å€¤åŸŸã«å¿œã˜ã¦ number_input or slider ã‚’ä½¿ã„åˆ†ã‘ï¼ˆãŠå¥½ã¿ï¼‰
        if "min" in spec and "max" in spec and "step" in spec:
            ui_values[name] = st.slider(
                label,
                min_value=spec["min"],
                max_value=spec["max"],
                step=spec["step"],
                value=default_value,
                key=f"param_{name}",
                format="%.6f" if spec["step"] < 1 else "%d"
            )
        else:
            ui_values[name] = st.slider(label, value=float(default_value), key=f"param_{name}")

        # session_state ã«ã‚‚å¸¸ã«åæ˜ ï¼ˆãƒ—ãƒªã‚»ãƒƒãƒˆâ†’æ‰‹å‹•èª¿æ•´ã®å¾€å¾©ã‚’è‡ªç„¶ã«ï¼‰
        st.session_state["params_ui"][name] = ui_values[name]

    # ã“ã“ã§ params ã‚’ model.run ã«æ¸¡ã™
    params = ui_values

# ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
try:
    _ensure_extdata_file(up, DEFAULT_XLS_NAME)
    model = _load_model_from_file(
        model_py_path if use_py_first else "",
        model_mdl_path if not use_py_first else "",
    )
except Exception as e:
    st.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
    st.stop()

# å‡ºåŠ›é¸æŠ
with st.container():
    st.header("4) å‡ºåŠ›ã®é¸æŠ")
    return_cols = st.multiselect(
        "è¡¨ç¤º/å‡ºåŠ›ã—ãŸã„å¤‰æ•°ï¼ˆã‚¹ãƒãƒ¼ã‚¯ã‚±ãƒ¼ã‚¹ï¼‰",
        DEFAULT_RETURN_COLS,
        default=DEFAULT_RETURN_COLS
    )

# è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿
st.divider()
st.header("5) è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ï¼ˆä»»æ„ï¼‰")
st.caption("CSV ä¾‹: `date,temperature,precipitation,flow,level`ï¼ˆæ—¥ä»˜ãƒ»æµé‡åˆ—ã ã‘ä½¿ã„ã¾ã™ï¼‰")
obs_file = st.file_uploader("è¦³æ¸¬CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["csv"], key="obs")
col1, col2, col3 = st.columns([1,1,1])
with col1:
    obs_date_col = st.text_input("æ—¥ä»˜åˆ—å", value="date")
with col2:
    obs_flow_col = st.text_input("æµé‡åˆ—å", value="flow")
with col3:
    obs_unit = st.selectbox("è¦³æ¸¬æµé‡ã®å˜ä½", options=["m3/day", "m3/s"], index=1)

max_lag_days = st.slider("å½¢çŠ¶æ¯”è¼ƒã®è¨±å®¹ãƒ©ã‚°ï¼ˆæ—¥ï¼‰", min_value=0, max_value=14, value=5)

run_btn = st.button("â–¶ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ", type="primary")

if run_btn:
    with st.spinner("è¨ˆç®—ä¸­..."):
        try:
            res = _run_simulation(model, params=params, timestamps=timestamps, return_cols=return_cols)
        except Exception as e:
            st.exception(e)
            st.stop()

    res = _build_model_datetime_index(res, pd.to_datetime(start_date))
    st.success("å®Œäº†ï¼")
    st.dataframe(res.head(), use_container_width=True)

    # ====== åŸºæœ¬ã‚°ãƒ©ãƒ•ï¼ˆé¸æŠåˆ—ã”ã¨ã«ã‚¿ãƒ–ï¼‰======
    st.subheader("ğŸ“ˆ æ™‚ç³»åˆ—ã‚°ãƒ©ãƒ•ï¼ˆãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ï¼‰")
    if not res.empty:
        tabs = st.tabs([c for c in res.columns])
        for tab, col in zip(tabs, res.columns):
            with tab:
                st.line_chart(res[[col]], height=300)

    # ====== è¦³æ¸¬ã¨ã®æ¯”è¼ƒï¼ˆriver_discharge_downstreamï¼‰======
    st.divider()
    st.subheader("ğŸ†š è¦³æ¸¬ã¨ã®æ¯”è¼ƒï¼šriver_discharge_downstream")
    if "river_discharge_downstream" not in res.columns:
        st.info("`river_discharge_downstream` ã‚’å‡ºåŠ›ã«å«ã‚ã¦ãã ã•ã„ã€‚")
    else:
        model_series = res["river_discharge_downstream"].rename("model_flow")

        if obs_file is not None:
            try:
                obs_df = _read_observed_csv(obs_file, obs_date_col, obs_flow_col, obs_unit)
            except Exception as e:
                st.error(f"è¦³æ¸¬CSVã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
                obs_df = None
        else:
            obs_df = None

        if obs_df is not None:
            merged = pd.concat([model_series, obs_df["obs_flow"]], axis=1).dropna()
            st.write("é‡ãªã‚ŠæœŸé–“:", merged.index.min().date(), "ã€œ", merged.index.max().date())

            # ãƒ©ã‚°æœ€é©åŒ–ï¼ˆå½¢çŠ¶æ¯”è¼ƒï¼‰
            lag, r_at_lag = _best_lag(merged["obs_flow"], merged["model_flow"], max_lag_days)
            st.caption(f"æœ€é©ãƒ©ã‚°ï¼ˆæ—¥ï¼‰: {lag}ï¼ˆç›¸é–¢ {r_at_lag:.3f}ï¼‰")
            if lag != 0:
                if lag > 0:   # ãƒ¢ãƒ‡ãƒ«ã‚’é…ã‚‰ã›ã‚‹ï¼ˆè¦³æ¸¬ã«å¯¾ã—ã¦ï¼‰
                    merged["model_flow_lag"] = merged["model_flow"].shift(lag)
                else:         # ãƒ¢ãƒ‡ãƒ«ã‚’é€²ã‚ã‚‹
                    merged["model_flow_lag"] = merged["model_flow"].shift(lag)
                merged = merged.dropna()
                model_plot_col = "model_flow_lag"
            else:
                model_plot_col = "model_flow"

            # æŒ‡æ¨™
            scores = _metrics(merged["obs_flow"], merged[model_plot_col])
            if scores:
                c1, c2, c3, c4, c5 = st.columns(5)
                c1.metric("RMSE (mÂ³/æ—¥)", f"{scores['RMSE']:.2f}")
                c2.metric("MAE (mÂ³/æ—¥)", f"{scores['MAE']:.2f}")
                c3.metric("Bias (mÂ³/æ—¥)", f"{scores['Mean Bias']:.2f}")
                c4.metric("ç›¸é–¢ r", f"{scores['Pearson r']:.3f}")
                c5.metric("NSE", f"{scores['NSE']:.3f}")

            # ç”Ÿãƒ‡ãƒ¼ã‚¿é‡ã­æãï¼ˆåŒä¸€å˜ä½ï¼šm3/æ—¥ï¼‰
            st.markdown("**é‡ã­æãï¼ˆå®Ÿå€¤ï¼‰**")
            plot_df = merged[[model_plot_col, "obs_flow"]].rename(columns={model_plot_col: "model", "obs_flow": "observed"})
            st.line_chart(plot_df, height=320, use_container_width=True)

            # å½¢çŠ¶æ¯”è¼ƒï¼ˆæ¨™æº–åŒ–ï¼‰
            st.markdown("**å½¢çŠ¶æ¯”è¼ƒï¼ˆæ¨™æº–åŒ–ï¼šå¹³å‡0ãƒ»åˆ†æ•£1ï¼‰**")
            z = plot_df.apply(lambda s: (s - s.mean()) / (s.std() if s.std()!=0 else 1))
            st.line_chart(z, height=320, use_container_width=True)

            with st.expander("ğŸ“¥ çµæœãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"):
                out = res.copy()
                out["obs_flow"] = obs_df["obs_flow"]
                out["model_flow"] = res["river_discharge_downstream"]
                if lag != 0:
                    out["model_flow_lag"] = out["model_flow"].shift(lag)
                csv = out.to_csv(index_label="date").encode("utf-8")
                st.download_button("ãƒ¢ãƒ‡ãƒ«ï¼‹è¦³æ¸¬ï¼ˆCSVï¼‰", data=csv, file_name="model_observed_merged.csv", mime="text/csv")
        else:
            st.info("è¦³æ¸¬CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€é‡ã­æãã¨ç²¾åº¦è©•ä¾¡ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")

    # ====== CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ======
    st.subheader("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«çµæœã‚’CSVã§ä¿å­˜")
    csv_model = res.to_csv(index_label="date").encode("utf-8")
    st.download_button("çµæœã‚’CSVã§ä¿å­˜", data=csv_model, file_name="simulation_output.csv", mime="text/csv")

st.divider()
with st.expander("ğŸ§© ãƒ’ãƒ³ãƒˆ & ã‚ˆãã‚ã‚‹ã¤ã¾ãšã"):
    st.markdown("""
- ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã® `river_discharge_downstream` ã¯ **mÂ³/æ—¥** ã‚’æƒ³å®šã€‚è¦³æ¸¬ãŒ **mÂ³/ç§’** ã®ã¨ãã¯å˜ä½é¸æŠã§è‡ªå‹•æ›ç®—ã—ã¾ã™ï¼ˆÃ—86400ï¼‰ã€‚
- ãƒ¢ãƒ‡ãƒ«æ™‚åˆ» 0 æ—¥ç›®ãŒ **ã€Œãƒ¢ãƒ‡ãƒ«é–‹å§‹æ—¥ã€** ã«å¯¾å¿œã—ã¾ã™ï¼ˆæ—¢å®šã¯ 2023-01-01ï¼‰ã€‚å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚
- å½¢çŠ¶æ¯”è¼ƒã¯ **ç›¸é–¢ãŒæœ€å¤§ã«ãªã‚‹ãƒ©ã‚°ï¼ˆÂ±Næ—¥ï¼‰** ã‚’è‡ªå‹•æ¢ç´¢ã—ã¾ã™ã€‚
- æŒ‡æ¨™: RMSE, MAE, å¹³å‡ãƒã‚¤ã‚¢ã‚¹, Pearson ç›¸é–¢, Nashâ€“Sutcliffe åŠ¹ç‡ï¼ˆNSEï¼‰ã€‚
- èµ·å‹•: `pip install streamlit pysd numpy pandas xlrd` â†’ `streamlit run app.py`
    """)